{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66.0M/66.0M [00:17<00:00, 3.97MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\srite\\.cache\\kagglehub\\datasets\\mlg-ulb\\creditcardfraud\\versions\\3\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56862     2]\n",
      " [   23    75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.97      0.77      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.99      0.88      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "File = \"C://Users//srite//OneDrive//credit_card.csv\"\n",
    "data = pd.read_csv(File)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('Class', axis=1)  # Features\n",
    "y = data['Class']               # Target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': randint(2, 5),\n",
    "    'min_samples_leaf': randint(1, 2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=20,  # Reduced iterations\n",
    "                                   cv=3, n_jobs=-1, verbose=2,\n",
    "                                   random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56855     9]\n",
      " [   41    57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.86      0.58      0.70        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.93      0.79      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "[[56855     9]\n",
      " [   41    57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.86      0.58      0.70        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.93      0.79      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=  27.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=  31.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=  32.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=70; total time=  33.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=70; total time=  29.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=70; total time=  30.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=3, n_estimators=68; total time=  27.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=3, n_estimators=68; total time=  26.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=3, n_estimators=68; total time=  28.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=4, n_estimators=60; total time=  23.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=4, n_estimators=60; total time=  24.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=4, n_estimators=60; total time=  25.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=71; total time=  28.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=71; total time=  31.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=71; total time=  32.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=3, n_estimators=73; total time=  30.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=3, n_estimators=73; total time=  28.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=3, n_estimators=73; total time=  30.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=51; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=51; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=51; total time=  14.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=61; total time=  24.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=61; total time=  24.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=61; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=93; total time=  25.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=93; total time=  26.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=3, n_estimators=93; total time=  25.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=76; total time=  32.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=76; total time=  36.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=76; total time=  36.7s\n",
      "Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 64}\n",
      "Random Forest Results:\n",
      "[[56862     2]\n",
      " [   23    75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.97      0.77      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.99      0.88      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load the dataset\n",
    "File = \"C://Users//srite//OneDrive//credit_card.csv\"\n",
    "data = pd.read_csv(File)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('Class', axis=1)  # Features\n",
    "y = data['Class']               # Target\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Alternative model: Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "logistic_y_pred = logistic_model.predict(X_test)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(confusion_matrix(y_test, logistic_y_pred))\n",
    "print(classification_report(y_test, logistic_y_pred))\n",
    "\n",
    "# Hyperparameter tuning for Random Forest using a smaller grid and iterations\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 100),\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': randint(2, 5),\n",
    "    'min_samples_leaf': randint(1, 2)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42, n_jobs=-1), param_distributions=param_dist, n_iter=10, cv=3, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best estimator\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
